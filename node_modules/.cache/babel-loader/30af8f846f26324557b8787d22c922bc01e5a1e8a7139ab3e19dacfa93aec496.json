{"ast":null,"code":"var _jsxFileName = \"D:\\\\ReactProjects\\\\Math-calculator\\\\math-calculator\\\\src\\\\sellami_components\\\\Inputs\\\\DropDownMenu.jsx\";\nimport React from 'react';\nimport Dropdown from 'react-bootstrap/Dropdown';\nimport { Link } from 'react-router-dom';\nimport \"../../Styles/DropDownMenu.css\";\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nfunction DropDownMenu(_ref) {\n  let {\n    selected\n  } = _ref;\n  // const [selected,setSelected] = useState(sel)\n  // // console.log(selected)\n  // const TitleList = {\n  //   \"AddMatrices\" : \"Add two matrices\",\n  //   \"DivScalarMatrix\" : \"Divide by Scalar\",\n  //   \"Dot_Product\" : \"Dot Product\",\n  //   \"one_norm\" : \"Matrix 1rst Norm\",\n  //   \"Euclidean_Norm\" : \"Matrix Euclidean Norm\",\n  //   \"Inifinity_norm\" : \"Matrix Infinity Norm\",\n  //   \"Manhattan_Norm\" : \"Vector Manhattan Norm\",\n  //   \"Euclidean_VNorm\" : \"Vector Euclidean Norm\",\n  //   \"Inifinity_VNorm\" : \"Vector Inifinity Norm\",\n  //   \"Lp_Norm\" : \"Vector Lp Norm\",\n  //   \"Manhattan_Distance\" : \"Matrix Manhattan Distance\",\n  //   \"Euclidean_Distance\" : \"Matrix Euclidean Distance\",\n  //   \"Infinity_Distance\" : \"Matrix Infinity Distance\",\n  //   \"Manhattan_VDistance\" : \"Vector Manhattan Distance\",\n  //   \"Euclidean_VDistance\" : \"Vector Euclidean Distance\",\n  //   \"Infinity_VDistance\" : \"Vector Infinity VDistance\",\n  //   \"Transpose\" : \"Matrix Transpose\",\n  //   \"Inverse_matrix\" : \"Matrix Inverse\",\n  //   \"Trace\" : \"Trace\",\n  //   \"Determinant\" : \"Determinant\",\n  //   \"Gaussian_Elm\" : \"Gaussian Elimination\",\n  //   \"Basis\" : \"Basis Extraction\",\n  //   \"Kernel\" : \"Kernel Extraction\",\n  //   \"Rank\" : \"Rank\",\n  //   \"Gram_Shmidt\" : \"Gram Shmidt\",\n  //   \"Particular_Solution\" : \"Particular Solution of Ax=b\",\n  //   \"General_solution\": \"General solution of Ax=b\",\n  //   \"LU_Solv\" : \"LU Decomposition for Solving a Linear equation\",\n  //   \"Eigenvalues_and_Eigenvectors\" : \"Eigenvalues and Eigenvectors\",\n  //   \"Diagonizable\" : \"Diagonizability Cecking\",\n  //   \"Convexity\" : \"Convexity Checking\",\n  //   \"Invertibility\" : \"Invertibility Checking\",\n  //   \"Orthogonality\" : \"Checking Orthogonality for a Subspace\",\n  //   \"Independency\" : \"Checking Independency for a Subspace\",\n  //   \"Angle\" : \"Angle between vectors\",\n  //   \"Projection\" : \"Orthogonal Projection\",\n  //   \"Affine_Projection\" : \"Orthogonal Projection onto an Affine subspace\",\n  //   \"Gradient_Descent\" : \"Gradient Descent for quadratic functions\",\n  //   \"Steepest_Gradient_Descent\" : \"Steepest Gradient Descent\",\n  //   \"Conjugate_Gradient_Descent\" : \"Conjugate Gradient Descent\",\n  //   \"Eigen_Decomposition\": \"Eigen Decomposition\",\n  //   \"LU\" : \"LU Decomposition\",\n  //   \"SVD\" : \"Singular Values Decomposition\",\n  //   \"Cholosky_Decomposition\" : \"Cholosky Decomposition\",\n  //   \"QR_Decomposition\" : \"QR Decomposition\",\n  //   \"Gradient_Linear_Regression\" : \"Univariante Linear Regression using Gradient Descent \",\n  //   \"MulMatrcies\" : \"Hadamard Product\",\n  //   \"MulScalarMatrix\" : \"Scalar Multiplication\",\n  //   \"SubMatrcies\" : \"Subtract two matrcies\",\n\n  // }\n  const TitleList = {\n    AddMatrices: \"Matrix Addition\",\n    DivScalarMatrix: \"Divide by Scalar\",\n    Dot_Product: \"Dot Product\",\n    one_norm: \"Matrix 1rst Norm\",\n    Euclidean_Norm: \"Matrix Euclidean Norm\",\n    Inifinity_norm: \"Matrix Infinity Norm\",\n    Manhattan_Norm: \"Vector Manhattan Norm\",\n    Euclidean_VNorm: \"Vector Euclidean Norm\",\n    Inifinity_VNorm: \"Vector Inifinity Norm\",\n    Lp_Norm: \"Vector Lp Norm\",\n    Manhattan_Distance: \"Matrix Manhattan Distance\",\n    Euclidean_Distance: \"Matrix Euclidean Distance\",\n    Infinity_Distance: \"Matrix Infinity Distance\",\n    Manhattan_VDistance: \"Vector Manhattan Distance\",\n    Euclidean_VDistance: \"Vector Euclidean Distance\",\n    Infinity_VDistance: \"Vector Infinity VDistance\",\n    Transpose: \"Matrix Transpose\",\n    Inverse_matrix: \"Matrix Inverse\",\n    Trace: \"Trace\",\n    Determinant: \"Determinant\",\n    Gaussian_Elm: \"Gaussian Elimination\",\n    Basis: \"Basis Extraction\",\n    Kernel: \"Matrix Kernel\",\n    Rank: \"Rank\",\n    Gram_Shmidt: \"Gram Shmidt\",\n    Particular_Solution: \"Particular Solution of Ax=b\",\n    General_solution: \"General solution of Ax=b\",\n    LU_Solv: \"LU Decomposition for Solving a Linear equation\",\n    Eigenvalues_and_Eigenvectors: \"Eigenvalues and Eigenvectors\",\n    Diagonizable: \"Diagonizability Cecking\",\n    Convexity: \"Convexity Checking\",\n    Invertibility: \"Invertibility Checking\",\n    Orthogonality: \"Checking Orthogonality for a Subspace\",\n    Independency: \"Checking Independency for a Subspace\",\n    Angle: \"Angle between vectors\",\n    Projection: \"Orthogonal Projection\",\n    Affine_Projection: \"Orthogonal Projection onto an Affine subspace\",\n    Gradient_Descent: \"Gradient Descent for quadratic functions\",\n    Steepest_Gradient_Descent: \"Steepest Gradient Descent\",\n    Conjugate_Gradient_Descent: \"Conjugate Gradient Descent\",\n    Eigen_Decomposition: \"Eigen Decomposition\",\n    LU: \"LU Decomposition\",\n    SVD: \"Singular Values Decomposition\",\n    Cholosky_Decomposition: \"Cholosky Decomposition\",\n    QR_Decomposition: \"QR Decomposition\",\n    Gradient_Linear_Regression: \"Univariante Linear Regression using Gradient Descent \",\n    MulMatrcies: \"Hadamard Product\",\n    MulScalarMatrix: \"Scalar Multiplication\",\n    SubMatrcies: \"Subtract two matrcies\"\n  };\n  const SubSubFunctions = [{\n    title: \"AddMatrices\",\n    description: \"This function adds two matrices together. The matrices must have the same dimensions in order to be added. The addition of matrices is commutative, meaning that the order in which the matrices are added does not affect the result.\",\n    img: AddMatrices_IMG\n  }, {\n    title: \"Dot_Product\",\n    description: \"This function computes the dot product of two vectors. The dot product of two vectors is a scalar that measures the similarity between the vectors. The dot product is commutative, meaning that the order in which the vectors are multiplied does not affect the result.\",\n    img: Dot_Product_IMG\n  }, {\n    title: \"MulMatrcies\",\n    description: \"This function performs element-wise multiplication of two matrices. The matrices must have the same dimensions in order to be multiplied in this way. The element-wise multiplication of matrices is distributive over addition, meaning that multiplying the sum of two matrices by a matrix is the same as multiplying each matrix by the matrix and then adding the results.\",\n    img: MulMatrcies_IMG\n  }, {\n    title: \"MulScalarMatrix\",\n    description: \"This function multiplies each element of a matrix by a scalar. The scalar can be any real number. The multiplication of a matrix by a scalar is distributive over addition, meaning that multiplying the sum of two matrices by a scalar is the same as multiplying each matrix by the scalar and then adding the results.\",\n    img: MulScalarMatrix_IMG\n  }, {\n    title: \"Manhattan_Distance\",\n    description: \"The Manhattan matrix distance, also known as the taxicab norm or the L1 norm, measures the sum of the absolute differences between corresponding elements of two matrices. It represents the total entry-wise deviation between the two matrices.\",\n    img: Manhattan_Distance_IMG\n  }, {\n    title: \"Euclidean_Distance\",\n    description: \"The Euclidean matrix distance, also known as the Frobenius norm, is the most commonly used matrix distance metric. It is the natural extension of the Euclidean distance for vectors to matrices. It measures the difference between corresponding elements of two matrices, taking into account all elements\",\n    img: Euclidean_Distance_IMG\n  }, {\n    title: \"Infinity_Distance\",\n    description: \"The infinity matrix distance, also known as the sup norm, measures the maximum absolute difference between corresponding elements of two matrices. It represents the largest entry-wise difference between the two matrices.\",\n    img: Infinity_Distance_IMG\n  },, {\n    title: \"one_norm\",\n    description: \"The Manhattan matrix norm, also known as the taxicab norm or the L1 norm, measures the sum of the absolute values of all the elements in the matrix. It represents the total magnitude of all the elements in the matrix.\",\n    img: one_norm_IMG\n  }, {\n    title: \"Euclidean_Norm\",\n    description: \"The Euclidean matrix norm, also known as the Frobenius norm, is the most common matrix norm. It is the square root of the sum of the squares of all the elements in the matrix. It represents the size of the matrix in its Euclidean space.\",\n    img: Euclidean_Norm_IMG\n  }, {\n    title: \"Inifinity_norm\",\n    description: \"The infinity matrix norm, also known as the sup norm, measures the largest absolute value of any element in the matrix. It represents the maximum magnitude of any element in the matrix.\",\n    img: Inifinity_norm_IMG\n  }, {\n    title: \"Transpose\",\n    description: \"The transpose of a matrix is created by swapping its rows and columns. Transposition has various applications in linear algebra, including matrix multiplication and solving systems of equations.\",\n    img: Transpose_IMG\n  }, {\n    title: \"Inverse_matrix\",\n    description: \"A non-singular matrix has an inverse matrix, which, when multiplied by the original matrix, produces the identity matrix. Invertible matrices play a crucial role in solving systems of linear equations.\",\n    img: Inverse_matrix_IMG\n  }, {\n    title: \"Trace\",\n    description: \"The trace of a square matrix is the sum of its diagonal elements. It provides insights into the matrix's structure and behavior, such as its stability and relationships to eigenvalues.\",\n    img: Trace_IMG\n  }, {\n    title: \"Determinant\",\n    description: \"The determinant of a square matrix is a scalar value that determines its volume, invertibility, and other properties. It has wide applications in various mathematical and computational contexts.\",\n    img: Determinant_IMG\n  }, {\n    title: \"Rank\",\n    description: \"The rank of a matrix is the number of linearly independent rows or columns it possesses. It indicates the matrix's dimensionality and its ability to represent linear relationships.\",\n    img: Rank_IMG\n  }, {\n    title: \"Orthogonality\",\n    description: \"Orthogonal matrices have perpendicular columns or rows, resulting in certain algebraic properties and applications in geometry and signal processing.\",\n    img: Orthogonality_IMG\n  }, {\n    title: \"Invertibility\",\n    description: \"A matrix is invertible if it has an inverse matrix, which, when multiplied by the original matrix, produces the identity matrix. Invertible matrices are crucial for solving systems of linear equations, performing matrix transformations, and analyzing linear systems.\",\n    img: Invertibility_IMG\n  }, {\n    title: \"Gram_Shmidt\",\n    description: \"Gram-Schmidt orthogonalization is an algorithm that constructs an orthonormal basis for a subspace. An orthonormal basis is a set of vectors that are mutually orthogonal (perpendicular) and have unit magnitude. This algorithm is widely used to find sets of vectors that are linearly independent and form a basis for a given subspace.\",\n    img: Gram_Shmidt_IMG\n  }, {\n    title: \"Basis\",\n    description: \"A basis for a vector space is a set of linearly independent vectors that span the entire space. This means that any vector in the space can be expressed as a linear combination of the vectors in the basis.\",\n    img: Basis_IMG\n  }, {\n    title: \"Gaussian_Elm\",\n    description: \"Gaussian elimination is a fundamental algorithm in linear algebra that is widely used for solving systems of linear equations and performing matrix operations. The algorithm works by systematically reducing a matrix to its row echelon form, which is a triangular matrix with zeros below the diagonal.\",\n    img: Gaussian_Elm_IMG\n  }, {\n    title: \"LU\",\n    description: \"LU decomposition factors a square matrix into a lower triangular matrix (L) and an upper triangular matrix (U). This decomposition is widely used for solving systems of linear equations and Gaussian elimination.\",\n    img: LU_IMG\n  }, {\n    title: \"SVD\",\n    description: \"SVD factorizes a rectangular matrix into three matrices: an orthogonal matrix (U), a diagonal matrix (Σ), and the transpose of another orthogonal matrix (V). SVD is powerful for data analysis, signal processing, and image compression.\",\n    img: SVD_IMG\n  }, {\n    title: \"QR_Decomposition\",\n    description: \"QR decomposition factors a rectangular matrix into an orthogonal matrix (Q) and an upper triangular matrix (R). This decomposition is useful for solving least squares  and analyzing data matrices.\",\n    img: QR_Decomposition_IMG\n  }, {\n    title: \"Cholosky_Decomposition\",\n    description: \"Cholesky decomposition factors a symmetric positive definite matrix into a lower triangular matrix with positive diagonal elements. This decomposition is efficient for solving systems of linear equations involving symmetric positive definite matrices.\",\n    img: Cholosky_Decomposition_IMG\n  }, {\n    title: \"Kernel\",\n    description: 'also known as the null space, is a fundamental concept in linear algebra that represents the set of all vectors that, when multiplied by a specific matrix, result in the zero vector. In simpler terms, it is the collection of vectors that get \"mapped to zero\" by the matrix transformation. The matrix kernel is denoted by Null(A) for a matrix A.',\n    img: Kernel_IMG\n  }, {\n    title: \"Manhattan_Norm\",\n    description: \"The Manhattan norm, also known as the L1 norm or the taxicab norm, measures the sum of the absolute values of all the elements in the vector. It represents the total magnitude of all the elements in the vector.\",\n    img: Manhattan_Norm_IMG\n  }, {\n    title: \"Euclidean_VNorm\",\n    description: \"The Euclidean norm, also known as the L2 norm, is the most commonly used vector norm. It is the natural extension of the Euclidean distance for vectors. The Euclidean norm measures the square root of the sum of the squares of all the elements in the vector. It represents the size of the vector in its Euclidean space.\",\n    img: Euclidean_VNorm_IMG\n  }, {\n    title: \"Inifinity_VNorm\",\n    description: \"The infinity norm measures the largest absolute value of any element in the vector. It represents the maximum magnitude of any element in the vector.\",\n    img: Inifinity_VNorm_IMG\n  }, {\n    title: \"Lp_Norm\",\n    description: \" also known as the Minkowski norm, is a generalization of the Euclidean norm (L2 norm) to higher dimensions. It is a measure of the magnitude or size of a vector in a p-dimensional space.\",\n    img: Lp_Norm_IMG\n  }, {\n    title: \"Manhattan_VDistance\",\n    description: \"The Manhattan distance, also known as the L1 distance or taxicab norm, measures the sum of the absolute values of the differences between corresponding elements of two vectors. It represents the total distance traveled if one were to move between two points along a grid pattern.\",\n    img: Manhattan_VDistance_IMG\n  }, {\n    title: \"Euclidean_VDistance\",\n    description: \"The Euclidean distance, also known as the L2 distance, is the most commonly used vector distance metric. It is a generalization of the Pythagorean theorem to higher dimensions and represents the straight-line distance between two points in Euclidean space.\",\n    img: Euclidean_VDistance_IMG\n  }, {\n    title: \"Infinity_VDistance\",\n    description: \"also known as the L∞ norm or supremum norm, is a mathematical measure that quantifies the separation or separation between two vectors in a vector space. It is a generalization of the maximum norm to higher dimensions and represents the largest absolute value of the differences between corresponding elements of two vectors.\",\n    img: Infinity_VDistance_IMG\n  }, {\n    title: \"Angle\",\n    description: 'The angle between two vectors in a vector space represents the measure of the \"turn\" between their directions. It quantifies the relative orientation of the vectors and provides insights into their alignment or opposition.',\n    img: Angle_IMG\n  }, {\n    title: \"Independency\",\n    description: \"Linear independence is a fundamental concept in linear algebra that describes the relationship between vectors in a vector space. A set of vectors is linearly independent if no vector in the set can be expressed as a linear combination of the other vectors.\",\n    img: Independency_IMG\n  }, {\n    title: \"Convexity\",\n    description: \"Convexity is a geometrical property that describes the shape of sets in a vector space. A set is convex if any line segment connecting two points within the set lies entirely within the set.\",\n    img: Convexity_IMG\n  }, {\n    title: \"Dot_Product\",\n    description: \"This function computes the dot product of two vectors. The dot product of two vectors is a scalar that measures the similarity between the vectors. The dot product is commutative, meaning that the order in which the vectors are multiplied does not affect the result.\",\n    img: Dot_Product_IMG\n  }, {\n    title: \"Orthogonality\",\n    description: \"Orthogonal matrices have perpendicular columns or rows, resulting in certain algebraic properties and applications in geometry and signal processing.\",\n    img: Orthogonality_IMG\n  }, {\n    title: \"Gram_Shmidt\",\n    description: \"Gram-Schmidt orthogonalization is an algorithm that constructs an orthonormal basis for a subspace. An orthonormal basis is a set of vectors that are mutually orthogonal (perpendicular) and have unit magnitude. This algorithm is widely used to find sets of vectors that are linearly independent and form a basis for a given subspace.\",\n    img: Gram_Shmidt_IMG\n  }, {\n    title: \"Projection\",\n    description: \"Projection is a geometrical operation that involves finding the shortest distance between a point and a line or plane. It is a fundamental concept in linear algebra that has various applications in geometry, physics, and engineering.\",\n    img: Projection_IMG\n  }, {\n    title: \"Affine_Projection\",\n    description: \"Affine projection is a generalization of projection that involves finding the shortest distance between a point and an affine subspace. It is a more general concept than projection onto a line or plane and has applications in various fields.\",\n    img: Affine_Projection_IMG\n  }, {\n    title: \"Particular_Solution\",\n    description: \"In linear algebra, a particular solution to a system of linear equations is a solution that satisfies the equations but may not be the only solution. A system of linear equations can have multiple particular solutions, one for each free variable in the system. The general solution to a system of linear equations is the set of all possible solutions, which can be expressed as a linear combination of the particular solutions.\",\n    img: Particular_Solution_IMG\n  }, {\n    title: \"General_solution\",\n    description: \"The general solution to a system of linear equations is the set of all possible solutions to the system. It is typically expressed as a linear combination of the basic solutions, which are the non-zero solutions that cannot be expressed as a linear combination of the other solutions. The general solution can be written in a variety of forms, such as a system of equations, a matrix equation, or a parametric equation.\",\n    img: General_solution_IMG\n  }, {\n    title: \"LU_Solv\",\n    description: \"is a numerical algorithm for solving systems of linear equations that involves factoring the coefficient matrix into an LU decomposition. The LU decomposition is a factorization of a matrix A into the product of two matrices, L and U, where L is a lower triangular matrix and U is an upper triangular matrix with ones on its diagonal. The LU_Solv algorithm uses this factorization to efficiently solve for the solution vector x to the system Ax = b.\",\n    img: LU_Solv_IMG\n  }, {\n    title: \"Diagonizable\",\n    description: \"A matrix A is said to be diagonalizable if there exists a non-singular matrix P such that PAP^-1 is a diagonal matrix. In other words, a diagonalizable matrix can be transformed into a diagonal matrix by a similarity transformation. The diagonal elements of the diagonal matrix are the eigenvalues of the original matrix A.\",\n    img: Diagonizable_IMG\n  }, {\n    title: \"Eigenvalues_and_Eigenvectors\",\n    description: \"Eigenvalues and eigenvectors are fundamental concepts in linear algebra that describe the behavior of linear transformations. An eigenvalue of a matrix A is a scalar λ such that there exists a non-zero vector v that satisfies the equation Av = λv. The vector v is called an eigenvector of A corresponding to the eigenvalue λ. Eigenvalues and eigenvectors have a wide range of applications in various fields, including physics, engineering, and computer science.\",\n    img: Eigenvalues_and_Eigenvectors_IMG\n  }, {\n    title: \"Gradient_Descent\",\n    description: \"Gradient descent is a general optimization algorithm for finding the minimum of a function. It works by iteratively moving in the direction of the negative gradient of the function, which is the direction of steepest descent. The algorithm stops when it reaches a minimum or reaches a predefined stopping criterion.\",\n    img: Gradient_Descent_IMG\n  }, {\n    title: \"Conjugate_Gradient_Descent\",\n    description: \"Conjugate gradient descent is an iterative optimization algorithm for solving a system of linear equations or minimizing a quadratic function. It is a more efficient variant of the gradient descent algorithm, which can converge to the optimal solution in a finite number of steps for certain types of .\",\n    img: Conjugate_Gradient_Descent_IMG\n  }, {\n    title: \"Steepest_Gradient_Descent\",\n    description: \"Steepest gradient descent is a specific implementation of the gradient descent algorithm that moves in the direction of the negative gradient of the function at each iteration. It is a simple and efficient algorithm but may not converge to the optimal solution for certain types of .\",\n    img: Steepest_Gradient_Descent_IMG\n  }, {\n    title: \"Gradient_Linear_Regression\",\n    description: \"Gradient linear regression is a statistical method for fitting a linear model to data by minimizing the sum of the squared residuals. It is a common technique for solving linear regression  and is widely used in various fields, including machine learning, economics, and finance.\",\n    img: Gradient_Linear_Regression_IMG\n  }];\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    children: /*#__PURE__*/_jsxDEV(Dropdown, {\n      className: \"w-100 \",\n      children: [/*#__PURE__*/_jsxDEV(Dropdown.Toggle, {\n        className: \" dropdown_btn\",\n        variant: \"success\",\n        id: \"dropdown-basic\",\n        children: selected == null ? \"Choose a Function \" : TitleList[selected]\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 440,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(Dropdown.Menu, {\n        className: \"dropdown_menu\",\n        children: Object.entries(TitleList).map(_ref2 => {\n          let [keyV, value] = _ref2;\n          return /*#__PURE__*/_jsxDEV(Dropdown.Item, {\n            className: selected === keyV ? \"active\" : \"\",\n            children: /*#__PURE__*/_jsxDEV(Link, {\n              to: '/Calculator/' + keyV,\n              className: \"d-flex w-100\",\n              children: /*#__PURE__*/_jsxDEV(\"span\", {\n                children: value\n              }, void 0, false, {\n                fileName: _jsxFileName,\n                lineNumber: 451,\n                columnNumber: 13\n              }, this)\n            }, void 0, false, {\n              fileName: _jsxFileName,\n              lineNumber: 450,\n              columnNumber: 11\n            }, this)\n          }, keyV, false, {\n            fileName: _jsxFileName,\n            lineNumber: 449,\n            columnNumber: 18\n          }, this);\n        })\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 447,\n        columnNumber: 9\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 439,\n      columnNumber: 9\n    }, this)\n  }, void 0, false, {\n    fileName: _jsxFileName,\n    lineNumber: 438,\n    columnNumber: 5\n  }, this);\n}\n_c = DropDownMenu;\nexport default DropDownMenu;\nvar _c;\n$RefreshReg$(_c, \"DropDownMenu\");","map":{"version":3,"names":["React","Dropdown","Link","jsxDEV","_jsxDEV","DropDownMenu","_ref","selected","TitleList","AddMatrices","DivScalarMatrix","Dot_Product","one_norm","Euclidean_Norm","Inifinity_norm","Manhattan_Norm","Euclidean_VNorm","Inifinity_VNorm","Lp_Norm","Manhattan_Distance","Euclidean_Distance","Infinity_Distance","Manhattan_VDistance","Euclidean_VDistance","Infinity_VDistance","Transpose","Inverse_matrix","Trace","Determinant","Gaussian_Elm","Basis","Kernel","Rank","Gram_Shmidt","Particular_Solution","General_solution","LU_Solv","Eigenvalues_and_Eigenvectors","Diagonizable","Convexity","Invertibility","Orthogonality","Independency","Angle","Projection","Affine_Projection","Gradient_Descent","Steepest_Gradient_Descent","Conjugate_Gradient_Descent","Eigen_Decomposition","LU","SVD","Cholosky_Decomposition","QR_Decomposition","Gradient_Linear_Regression","MulMatrcies","MulScalarMatrix","SubMatrcies","SubSubFunctions","title","description","img","AddMatrices_IMG","Dot_Product_IMG","MulMatrcies_IMG","MulScalarMatrix_IMG","Manhattan_Distance_IMG","Euclidean_Distance_IMG","Infinity_Distance_IMG","one_norm_IMG","Euclidean_Norm_IMG","Inifinity_norm_IMG","Transpose_IMG","Inverse_matrix_IMG","Trace_IMG","Determinant_IMG","Rank_IMG","Orthogonality_IMG","Invertibility_IMG","Gram_Shmidt_IMG","Basis_IMG","Gaussian_Elm_IMG","LU_IMG","SVD_IMG","QR_Decomposition_IMG","Cholosky_Decomposition_IMG","Kernel_IMG","Manhattan_Norm_IMG","Euclidean_VNorm_IMG","Inifinity_VNorm_IMG","Lp_Norm_IMG","Manhattan_VDistance_IMG","Euclidean_VDistance_IMG","Infinity_VDistance_IMG","Angle_IMG","Independency_IMG","Convexity_IMG","Projection_IMG","Affine_Projection_IMG","Particular_Solution_IMG","General_solution_IMG","LU_Solv_IMG","Diagonizable_IMG","Eigenvalues_and_Eigenvectors_IMG","Gradient_Descent_IMG","Conjugate_Gradient_Descent_IMG","Steepest_Gradient_Descent_IMG","Gradient_Linear_Regression_IMG","children","className","Toggle","variant","id","fileName","_jsxFileName","lineNumber","columnNumber","Menu","Object","entries","map","_ref2","keyV","value","Item","to","_c","$RefreshReg$"],"sources":["D:/ReactProjects/Math-calculator/math-calculator/src/sellami_components/Inputs/DropDownMenu.jsx"],"sourcesContent":["import React from 'react'\r\nimport Dropdown from 'react-bootstrap/Dropdown';\r\nimport {Link} from 'react-router-dom'\r\nimport \"../../Styles/DropDownMenu.css\"\r\n\r\nfunction DropDownMenu ({selected}){\r\n  // const [selected,setSelected] = useState(sel)\r\n  // // console.log(selected)\r\n  // const TitleList = {\r\n  //   \"AddMatrices\" : \"Add two matrices\",\r\n  //   \"DivScalarMatrix\" : \"Divide by Scalar\",\r\n  //   \"Dot_Product\" : \"Dot Product\",\r\n  //   \"one_norm\" : \"Matrix 1rst Norm\",\r\n  //   \"Euclidean_Norm\" : \"Matrix Euclidean Norm\",\r\n  //   \"Inifinity_norm\" : \"Matrix Infinity Norm\",\r\n  //   \"Manhattan_Norm\" : \"Vector Manhattan Norm\",\r\n  //   \"Euclidean_VNorm\" : \"Vector Euclidean Norm\",\r\n  //   \"Inifinity_VNorm\" : \"Vector Inifinity Norm\",\r\n  //   \"Lp_Norm\" : \"Vector Lp Norm\",\r\n  //   \"Manhattan_Distance\" : \"Matrix Manhattan Distance\",\r\n  //   \"Euclidean_Distance\" : \"Matrix Euclidean Distance\",\r\n  //   \"Infinity_Distance\" : \"Matrix Infinity Distance\",\r\n  //   \"Manhattan_VDistance\" : \"Vector Manhattan Distance\",\r\n  //   \"Euclidean_VDistance\" : \"Vector Euclidean Distance\",\r\n  //   \"Infinity_VDistance\" : \"Vector Infinity VDistance\",\r\n  //   \"Transpose\" : \"Matrix Transpose\",\r\n  //   \"Inverse_matrix\" : \"Matrix Inverse\",\r\n  //   \"Trace\" : \"Trace\",\r\n  //   \"Determinant\" : \"Determinant\",\r\n  //   \"Gaussian_Elm\" : \"Gaussian Elimination\",\r\n  //   \"Basis\" : \"Basis Extraction\",\r\n  //   \"Kernel\" : \"Kernel Extraction\",\r\n  //   \"Rank\" : \"Rank\",\r\n  //   \"Gram_Shmidt\" : \"Gram Shmidt\",\r\n  //   \"Particular_Solution\" : \"Particular Solution of Ax=b\",\r\n  //   \"General_solution\": \"General solution of Ax=b\",\r\n  //   \"LU_Solv\" : \"LU Decomposition for Solving a Linear equation\",\r\n  //   \"Eigenvalues_and_Eigenvectors\" : \"Eigenvalues and Eigenvectors\",\r\n  //   \"Diagonizable\" : \"Diagonizability Cecking\",\r\n  //   \"Convexity\" : \"Convexity Checking\",\r\n  //   \"Invertibility\" : \"Invertibility Checking\",\r\n  //   \"Orthogonality\" : \"Checking Orthogonality for a Subspace\",\r\n  //   \"Independency\" : \"Checking Independency for a Subspace\",\r\n  //   \"Angle\" : \"Angle between vectors\",\r\n  //   \"Projection\" : \"Orthogonal Projection\",\r\n  //   \"Affine_Projection\" : \"Orthogonal Projection onto an Affine subspace\",\r\n  //   \"Gradient_Descent\" : \"Gradient Descent for quadratic functions\",\r\n  //   \"Steepest_Gradient_Descent\" : \"Steepest Gradient Descent\",\r\n  //   \"Conjugate_Gradient_Descent\" : \"Conjugate Gradient Descent\",\r\n  //   \"Eigen_Decomposition\": \"Eigen Decomposition\",\r\n  //   \"LU\" : \"LU Decomposition\",\r\n  //   \"SVD\" : \"Singular Values Decomposition\",\r\n  //   \"Cholosky_Decomposition\" : \"Cholosky Decomposition\",\r\n  //   \"QR_Decomposition\" : \"QR Decomposition\",\r\n  //   \"Gradient_Linear_Regression\" : \"Univariante Linear Regression using Gradient Descent \",\r\n  //   \"MulMatrcies\" : \"Hadamard Product\",\r\n  //   \"MulScalarMatrix\" : \"Scalar Multiplication\",\r\n  //   \"SubMatrcies\" : \"Subtract two matrcies\",\r\n\r\n  // }\r\n  const TitleList = {\r\n    AddMatrices: \"Matrix Addition\",\r\n    DivScalarMatrix: \"Divide by Scalar\",\r\n    Dot_Product: \"Dot Product\",\r\n    one_norm: \"Matrix 1rst Norm\",\r\n    Euclidean_Norm: \"Matrix Euclidean Norm\",\r\n    Inifinity_norm: \"Matrix Infinity Norm\",\r\n    Manhattan_Norm: \"Vector Manhattan Norm\",\r\n    Euclidean_VNorm: \"Vector Euclidean Norm\",\r\n    Inifinity_VNorm: \"Vector Inifinity Norm\",\r\n    Lp_Norm: \"Vector Lp Norm\",\r\n    Manhattan_Distance: \"Matrix Manhattan Distance\",\r\n    Euclidean_Distance: \"Matrix Euclidean Distance\",\r\n    Infinity_Distance: \"Matrix Infinity Distance\",\r\n    Manhattan_VDistance: \"Vector Manhattan Distance\",\r\n    Euclidean_VDistance: \"Vector Euclidean Distance\",\r\n    Infinity_VDistance: \"Vector Infinity VDistance\",\r\n    Transpose: \"Matrix Transpose\",\r\n    Inverse_matrix: \"Matrix Inverse\",\r\n    Trace: \"Trace\",\r\n    Determinant: \"Determinant\",\r\n    Gaussian_Elm: \"Gaussian Elimination\",\r\n    Basis: \"Basis Extraction\",\r\n    Kernel: \"Matrix Kernel\",\r\n    Rank: \"Rank\",\r\n    Gram_Shmidt: \"Gram Shmidt\",\r\n    Particular_Solution: \"Particular Solution of Ax=b\",\r\n    General_solution: \"General solution of Ax=b\",\r\n    LU_Solv: \"LU Decomposition for Solving a Linear equation\",\r\n    Eigenvalues_and_Eigenvectors: \"Eigenvalues and Eigenvectors\",\r\n    Diagonizable: \"Diagonizability Cecking\",\r\n    Convexity: \"Convexity Checking\",\r\n    Invertibility: \"Invertibility Checking\",\r\n    Orthogonality: \"Checking Orthogonality for a Subspace\",\r\n    Independency: \"Checking Independency for a Subspace\",\r\n    Angle: \"Angle between vectors\",\r\n    Projection: \"Orthogonal Projection\",\r\n    Affine_Projection: \"Orthogonal Projection onto an Affine subspace\",\r\n    Gradient_Descent: \"Gradient Descent for quadratic functions\",\r\n    Steepest_Gradient_Descent: \"Steepest Gradient Descent\",\r\n    Conjugate_Gradient_Descent: \"Conjugate Gradient Descent\",\r\n    Eigen_Decomposition: \"Eigen Decomposition\",\r\n    LU: \"LU Decomposition\",\r\n    SVD: \"Singular Values Decomposition\",\r\n    Cholosky_Decomposition: \"Cholosky Decomposition\",\r\n    QR_Decomposition: \"QR Decomposition\",\r\n    Gradient_Linear_Regression:\r\n      \"Univariante Linear Regression using Gradient Descent \",\r\n    MulMatrcies: \"Hadamard Product\",\r\n    MulScalarMatrix: \"Scalar Multiplication\",\r\n    SubMatrcies: \"Subtract two matrcies\",\r\n  };\r\n  const SubSubFunctions = [\r\n    \r\n      \r\n        {\r\n          title: \"AddMatrices\",\r\n          description:\r\n            \"This function adds two matrices together. The matrices must have the same dimensions in order to be added. The addition of matrices is commutative, meaning that the order in which the matrices are added does not affect the result.\",\r\n          img: AddMatrices_IMG,\r\n        },\r\n        {\r\n          title: \"Dot_Product\",\r\n          description:\r\n            \"This function computes the dot product of two vectors. The dot product of two vectors is a scalar that measures the similarity between the vectors. The dot product is commutative, meaning that the order in which the vectors are multiplied does not affect the result.\",\r\n          img: Dot_Product_IMG,\r\n        },\r\n        {\r\n          title: \"MulMatrcies\",\r\n          description:\r\n            \"This function performs element-wise multiplication of two matrices. The matrices must have the same dimensions in order to be multiplied in this way. The element-wise multiplication of matrices is distributive over addition, meaning that multiplying the sum of two matrices by a matrix is the same as multiplying each matrix by the matrix and then adding the results.\",\r\n          img: MulMatrcies_IMG,\r\n        },\r\n        {\r\n          title: \"MulScalarMatrix\",\r\n          description:\r\n            \"This function multiplies each element of a matrix by a scalar. The scalar can be any real number. The multiplication of a matrix by a scalar is distributive over addition, meaning that multiplying the sum of two matrices by a scalar is the same as multiplying each matrix by the scalar and then adding the results.\",\r\n          img: MulScalarMatrix_IMG,\r\n        },\r\n     \r\n    \r\n    \r\n      \r\n        {\r\n          title: \"Manhattan_Distance\",\r\n          description:\r\n            \"The Manhattan matrix distance, also known as the taxicab norm or the L1 norm, measures the sum of the absolute differences between corresponding elements of two matrices. It represents the total entry-wise deviation between the two matrices.\",\r\n          img: Manhattan_Distance_IMG,\r\n        },\r\n        {\r\n          title: \"Euclidean_Distance\",\r\n          description:\r\n            \"The Euclidean matrix distance, also known as the Frobenius norm, is the most commonly used matrix distance metric. It is the natural extension of the Euclidean distance for vectors to matrices. It measures the difference between corresponding elements of two matrices, taking into account all elements\",\r\n          img: Euclidean_Distance_IMG,\r\n        },\r\n        {\r\n          title: \"Infinity_Distance\",\r\n          description:\r\n            \"The infinity matrix distance, also known as the sup norm, measures the maximum absolute difference between corresponding elements of two matrices. It represents the largest entry-wise difference between the two matrices.\",\r\n          img: Infinity_Distance_IMG,\r\n        },\r\n      ,\r\n      \r\n        {\r\n          title: \"one_norm\",\r\n          description:\r\n            \"The Manhattan matrix norm, also known as the taxicab norm or the L1 norm, measures the sum of the absolute values of all the elements in the matrix. It represents the total magnitude of all the elements in the matrix.\",\r\n          img: one_norm_IMG,\r\n        },\r\n        {\r\n          title: \"Euclidean_Norm\",\r\n          description:\r\n            \"The Euclidean matrix norm, also known as the Frobenius norm, is the most common matrix norm. It is the square root of the sum of the squares of all the elements in the matrix. It represents the size of the matrix in its Euclidean space.\",\r\n          img: Euclidean_Norm_IMG,\r\n        },\r\n        {\r\n          title: \"Inifinity_norm\",\r\n          description:\r\n            \"The infinity matrix norm, also known as the sup norm, measures the largest absolute value of any element in the matrix. It represents the maximum magnitude of any element in the matrix.\",\r\n          img: Inifinity_norm_IMG,\r\n        }\r\n      ,\r\n      \r\n        {\r\n          title: \"Transpose\",\r\n          description:\r\n            \"The transpose of a matrix is created by swapping its rows and columns. Transposition has various applications in linear algebra, including matrix multiplication and solving systems of equations.\",\r\n          img: Transpose_IMG,\r\n        },\r\n        {\r\n          title: \"Inverse_matrix\",\r\n          description:\r\n            \"A non-singular matrix has an inverse matrix, which, when multiplied by the original matrix, produces the identity matrix. Invertible matrices play a crucial role in solving systems of linear equations.\",\r\n          img: Inverse_matrix_IMG,\r\n        },\r\n        {\r\n          title: \"Trace\",\r\n          description:\r\n            \"The trace of a square matrix is the sum of its diagonal elements. It provides insights into the matrix's structure and behavior, such as its stability and relationships to eigenvalues.\",\r\n          img: Trace_IMG,\r\n        },\r\n        {\r\n          title: \"Determinant\",\r\n          description:\r\n            \"The determinant of a square matrix is a scalar value that determines its volume, invertibility, and other properties. It has wide applications in various mathematical and computational contexts.\",\r\n          img: Determinant_IMG,\r\n        },\r\n        {\r\n          title: \"Rank\",\r\n          description:\r\n            \"The rank of a matrix is the number of linearly independent rows or columns it possesses. It indicates the matrix's dimensionality and its ability to represent linear relationships.\",\r\n          img: Rank_IMG,\r\n        },\r\n        {\r\n          title: \"Orthogonality\",\r\n          description:\r\n            \"Orthogonal matrices have perpendicular columns or rows, resulting in certain algebraic properties and applications in geometry and signal processing.\",\r\n          img: Orthogonality_IMG,\r\n        },\r\n        {\r\n          title: \"Invertibility\",\r\n          description:\r\n            \"A matrix is invertible if it has an inverse matrix, which, when multiplied by the original matrix, produces the identity matrix. Invertible matrices are crucial for solving systems of linear equations, performing matrix transformations, and analyzing linear systems.\",\r\n          img: Invertibility_IMG,\r\n        },\r\n        {\r\n          title: \"Gram_Shmidt\",\r\n          description:\r\n            \"Gram-Schmidt orthogonalization is an algorithm that constructs an orthonormal basis for a subspace. An orthonormal basis is a set of vectors that are mutually orthogonal (perpendicular) and have unit magnitude. This algorithm is widely used to find sets of vectors that are linearly independent and form a basis for a given subspace.\",\r\n          img: Gram_Shmidt_IMG,\r\n        },\r\n      \r\n      \r\n        {\r\n          title: \"Basis\",\r\n          description:\r\n            \"A basis for a vector space is a set of linearly independent vectors that span the entire space. This means that any vector in the space can be expressed as a linear combination of the vectors in the basis.\",\r\n          img: Basis_IMG,\r\n        },\r\n        {\r\n          title: \"Gaussian_Elm\",\r\n          description:\r\n            \"Gaussian elimination is a fundamental algorithm in linear algebra that is widely used for solving systems of linear equations and performing matrix operations. The algorithm works by systematically reducing a matrix to its row echelon form, which is a triangular matrix with zeros below the diagonal.\",\r\n          img: Gaussian_Elm_IMG,\r\n        },\r\n        {\r\n          title: \"LU\",\r\n          description:\r\n            \"LU decomposition factors a square matrix into a lower triangular matrix (L) and an upper triangular matrix (U). This decomposition is widely used for solving systems of linear equations and Gaussian elimination.\",\r\n          img: LU_IMG,\r\n        },\r\n        {\r\n          title: \"SVD\",\r\n          description:\r\n            \"SVD factorizes a rectangular matrix into three matrices: an orthogonal matrix (U), a diagonal matrix (Σ), and the transpose of another orthogonal matrix (V). SVD is powerful for data analysis, signal processing, and image compression.\",\r\n          img: SVD_IMG,\r\n        },\r\n        {\r\n          title: \"QR_Decomposition\",\r\n          description:\r\n            \"QR decomposition factors a rectangular matrix into an orthogonal matrix (Q) and an upper triangular matrix (R). This decomposition is useful for solving least squares  and analyzing data matrices.\",\r\n          img: QR_Decomposition_IMG,\r\n        },\r\n        {\r\n          title: \"Cholosky_Decomposition\",\r\n          description:\r\n            \"Cholesky decomposition factors a symmetric positive definite matrix into a lower triangular matrix with positive diagonal elements. This decomposition is efficient for solving systems of linear equations involving symmetric positive definite matrices.\",\r\n          img: Cholosky_Decomposition_IMG,\r\n        },\r\n        {\r\n          title: \"Kernel\",\r\n          description:\r\n            'also known as the null space, is a fundamental concept in linear algebra that represents the set of all vectors that, when multiplied by a specific matrix, result in the zero vector. In simpler terms, it is the collection of vectors that get \"mapped to zero\" by the matrix transformation. The matrix kernel is denoted by Null(A) for a matrix A.',\r\n          img: Kernel_IMG,\r\n        },\r\n     \r\n    \r\n      \r\n        {\r\n          title: \"Manhattan_Norm\",\r\n          description:\r\n            \"The Manhattan norm, also known as the L1 norm or the taxicab norm, measures the sum of the absolute values of all the elements in the vector. It represents the total magnitude of all the elements in the vector.\",\r\n          img: Manhattan_Norm_IMG,\r\n        },\r\n        {\r\n          title: \"Euclidean_VNorm\",\r\n          description:\r\n            \"The Euclidean norm, also known as the L2 norm, is the most commonly used vector norm. It is the natural extension of the Euclidean distance for vectors. The Euclidean norm measures the square root of the sum of the squares of all the elements in the vector. It represents the size of the vector in its Euclidean space.\",\r\n          img: Euclidean_VNorm_IMG,\r\n        },\r\n        {\r\n          title: \"Inifinity_VNorm\",\r\n          description:\r\n            \"The infinity norm measures the largest absolute value of any element in the vector. It represents the maximum magnitude of any element in the vector.\",\r\n          img: Inifinity_VNorm_IMG,\r\n        },\r\n        {\r\n          title: \"Lp_Norm\",\r\n          description:\r\n            \" also known as the Minkowski norm, is a generalization of the Euclidean norm (L2 norm) to higher dimensions. It is a measure of the magnitude or size of a vector in a p-dimensional space.\",\r\n          img: Lp_Norm_IMG,\r\n        },\r\n      \r\n      \r\n        {\r\n          title: \"Manhattan_VDistance\",\r\n          description:\r\n            \"The Manhattan distance, also known as the L1 distance or taxicab norm, measures the sum of the absolute values of the differences between corresponding elements of two vectors. It represents the total distance traveled if one were to move between two points along a grid pattern.\",\r\n          img: Manhattan_VDistance_IMG,\r\n        },\r\n        {\r\n          title: \"Euclidean_VDistance\",\r\n          description:\r\n            \"The Euclidean distance, also known as the L2 distance, is the most commonly used vector distance metric. It is a generalization of the Pythagorean theorem to higher dimensions and represents the straight-line distance between two points in Euclidean space.\",\r\n          img: Euclidean_VDistance_IMG,\r\n        },\r\n        {\r\n          title: \"Infinity_VDistance\",\r\n          description:\r\n            \"also known as the L∞ norm or supremum norm, is a mathematical measure that quantifies the separation or separation between two vectors in a vector space. It is a generalization of the maximum norm to higher dimensions and represents the largest absolute value of the differences between corresponding elements of two vectors.\",\r\n          img: Infinity_VDistance_IMG,\r\n        },\r\n      \r\n      \r\n        {\r\n          title: \"Angle\",\r\n          description:\r\n            'The angle between two vectors in a vector space represents the measure of the \"turn\" between their directions. It quantifies the relative orientation of the vectors and provides insights into their alignment or opposition.',\r\n          img: Angle_IMG,\r\n        },\r\n        {\r\n          title: \"Independency\",\r\n          description:\r\n            \"Linear independence is a fundamental concept in linear algebra that describes the relationship between vectors in a vector space. A set of vectors is linearly independent if no vector in the set can be expressed as a linear combination of the other vectors.\",\r\n          img: Independency_IMG,\r\n        },\r\n        {\r\n          title: \"Convexity\",\r\n          description:\r\n            \"Convexity is a geometrical property that describes the shape of sets in a vector space. A set is convex if any line segment connecting two points within the set lies entirely within the set.\",\r\n          img: Convexity_IMG,\r\n        },\r\n        {\r\n          title: \"Dot_Product\",\r\n          description:\r\n            \"This function computes the dot product of two vectors. The dot product of two vectors is a scalar that measures the similarity between the vectors. The dot product is commutative, meaning that the order in which the vectors are multiplied does not affect the result.\",\r\n          img: Dot_Product_IMG,\r\n        },\r\n        {\r\n          title: \"Orthogonality\",\r\n          description:\r\n            \"Orthogonal matrices have perpendicular columns or rows, resulting in certain algebraic properties and applications in geometry and signal processing.\",\r\n          img: Orthogonality_IMG,\r\n        },\r\n        {\r\n          title: \"Gram_Shmidt\",\r\n          description:\r\n            \"Gram-Schmidt orthogonalization is an algorithm that constructs an orthonormal basis for a subspace. An orthonormal basis is a set of vectors that are mutually orthogonal (perpendicular) and have unit magnitude. This algorithm is widely used to find sets of vectors that are linearly independent and form a basis for a given subspace.\",\r\n          img: Gram_Shmidt_IMG,\r\n        },\r\n        {\r\n          title: \"Projection\",\r\n          description:\r\n            \"Projection is a geometrical operation that involves finding the shortest distance between a point and a line or plane. It is a fundamental concept in linear algebra that has various applications in geometry, physics, and engineering.\",\r\n          img: Projection_IMG,\r\n        },\r\n        {\r\n          title: \"Affine_Projection\",\r\n          description:\r\n            \"Affine projection is a generalization of projection that involves finding the shortest distance between a point and an affine subspace. It is a more general concept than projection onto a line or plane and has applications in various fields.\",\r\n          img: Affine_Projection_IMG,\r\n        },\r\n      \r\n    \r\n      \r\n        {\r\n          title: \"Particular_Solution\",\r\n          description:\r\n            \"In linear algebra, a particular solution to a system of linear equations is a solution that satisfies the equations but may not be the only solution. A system of linear equations can have multiple particular solutions, one for each free variable in the system. The general solution to a system of linear equations is the set of all possible solutions, which can be expressed as a linear combination of the particular solutions.\",\r\n          img: Particular_Solution_IMG,\r\n        },\r\n        {\r\n          title: \"General_solution\",\r\n          description:\r\n            \"The general solution to a system of linear equations is the set of all possible solutions to the system. It is typically expressed as a linear combination of the basic solutions, which are the non-zero solutions that cannot be expressed as a linear combination of the other solutions. The general solution can be written in a variety of forms, such as a system of equations, a matrix equation, or a parametric equation.\",\r\n          img: General_solution_IMG,\r\n        },\r\n        {\r\n          title: \"LU_Solv\",\r\n          description:\r\n            \"is a numerical algorithm for solving systems of linear equations that involves factoring the coefficient matrix into an LU decomposition. The LU decomposition is a factorization of a matrix A into the product of two matrices, L and U, where L is a lower triangular matrix and U is an upper triangular matrix with ones on its diagonal. The LU_Solv algorithm uses this factorization to efficiently solve for the solution vector x to the system Ax = b.\",\r\n          img: LU_Solv_IMG,\r\n        },\r\n      \r\n  \r\n        {\r\n          title: \"Diagonizable\",\r\n          description:\r\n            \"A matrix A is said to be diagonalizable if there exists a non-singular matrix P such that PAP^-1 is a diagonal matrix. In other words, a diagonalizable matrix can be transformed into a diagonal matrix by a similarity transformation. The diagonal elements of the diagonal matrix are the eigenvalues of the original matrix A.\",\r\n          img: Diagonizable_IMG,\r\n        },\r\n        {\r\n          title: \"Eigenvalues_and_Eigenvectors\",\r\n          description:\r\n            \"Eigenvalues and eigenvectors are fundamental concepts in linear algebra that describe the behavior of linear transformations. An eigenvalue of a matrix A is a scalar λ such that there exists a non-zero vector v that satisfies the equation Av = λv. The vector v is called an eigenvector of A corresponding to the eigenvalue λ. Eigenvalues and eigenvectors have a wide range of applications in various fields, including physics, engineering, and computer science.\",\r\n          img: Eigenvalues_and_Eigenvectors_IMG,\r\n        },\r\n      \r\n      \r\n        {\r\n          title: \"Gradient_Descent\",\r\n          description:\r\n            \"Gradient descent is a general optimization algorithm for finding the minimum of a function. It works by iteratively moving in the direction of the negative gradient of the function, which is the direction of steepest descent. The algorithm stops when it reaches a minimum or reaches a predefined stopping criterion.\",\r\n          img: Gradient_Descent_IMG,\r\n        },\r\n        {\r\n          title: \"Conjugate_Gradient_Descent\",\r\n          description:\r\n            \"Conjugate gradient descent is an iterative optimization algorithm for solving a system of linear equations or minimizing a quadratic function. It is a more efficient variant of the gradient descent algorithm, which can converge to the optimal solution in a finite number of steps for certain types of .\",\r\n          img: Conjugate_Gradient_Descent_IMG,\r\n        },\r\n        {\r\n          title: \"Steepest_Gradient_Descent\",\r\n          description:\r\n            \"Steepest gradient descent is a specific implementation of the gradient descent algorithm that moves in the direction of the negative gradient of the function at each iteration. It is a simple and efficient algorithm but may not converge to the optimal solution for certain types of .\",\r\n          img: Steepest_Gradient_Descent_IMG,\r\n        },\r\n        {\r\n          title: \"Gradient_Linear_Regression\",\r\n          description:\r\n            \"Gradient linear regression is a statistical method for fitting a linear model to data by minimizing the sum of the squared residuals. It is a common technique for solving linear regression  and is widely used in various fields, including machine learning, economics, and finance.\",\r\n          img: Gradient_Linear_Regression_IMG,\r\n        },\r\n     \r\n      ]\r\n  return (\r\n    \r\n    <div>\r\n        <Dropdown className='w-100 '>\r\n        <Dropdown.Toggle className=' dropdown_btn' variant=\"success\" id=\"dropdown-basic\">\r\n            {\r\n              selected == null ? \"Choose a Function \" : TitleList[selected]\r\n            }\r\n            \r\n        </Dropdown.Toggle>\r\n\r\n        <Dropdown.Menu className='dropdown_menu'>\r\n        {Object.entries(TitleList).map(([keyV, value])=>{\r\n          return <Dropdown.Item key={keyV}  className={selected===keyV? \"active\" : \"\"} >\r\n          <Link to={'/Calculator/'+keyV} className='d-flex w-100'>\r\n            <span>\r\n            {value}\r\n            </span>\r\n\r\n          </Link>\r\n          </Dropdown.Item>\r\n        })}\r\n{/*         \r\n            <Dropdown.Item  className={selected===\"AddMatrices\" ? \"active\" : \"\"} >\r\n              <Link to='/Calculator/AddMatrices' className='d-flex w-100'>Add two matrices</Link>\r\n              </Dropdown.Item>\r\n            <Dropdown.Item href=\"#/action-2\">Subtract</Dropdown.Item>\r\n            <Dropdown.Item href=\"#/action-3\">multiplication</Dropdown.Item> */}\r\n        </Dropdown.Menu>\r\n        </Dropdown>\r\n    </div>\r\n  )\r\n}\r\n\r\nexport default DropDownMenu"],"mappings":";AAAA,OAAOA,KAAK,MAAM,OAAO;AACzB,OAAOC,QAAQ,MAAM,0BAA0B;AAC/C,SAAQC,IAAI,QAAO,kBAAkB;AACrC,OAAO,+BAA+B;AAAA,SAAAC,MAAA,IAAAC,OAAA;AAEtC,SAASC,YAAYA,CAAAC,IAAA,EAAa;EAAA,IAAX;IAACC;EAAQ,CAAC,GAAAD,IAAA;EAC/B;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;EACA;;EAEA;EACA,MAAME,SAAS,GAAG;IAChBC,WAAW,EAAE,iBAAiB;IAC9BC,eAAe,EAAE,kBAAkB;IACnCC,WAAW,EAAE,aAAa;IAC1BC,QAAQ,EAAE,kBAAkB;IAC5BC,cAAc,EAAE,uBAAuB;IACvCC,cAAc,EAAE,sBAAsB;IACtCC,cAAc,EAAE,uBAAuB;IACvCC,eAAe,EAAE,uBAAuB;IACxCC,eAAe,EAAE,uBAAuB;IACxCC,OAAO,EAAE,gBAAgB;IACzBC,kBAAkB,EAAE,2BAA2B;IAC/CC,kBAAkB,EAAE,2BAA2B;IAC/CC,iBAAiB,EAAE,0BAA0B;IAC7CC,mBAAmB,EAAE,2BAA2B;IAChDC,mBAAmB,EAAE,2BAA2B;IAChDC,kBAAkB,EAAE,2BAA2B;IAC/CC,SAAS,EAAE,kBAAkB;IAC7BC,cAAc,EAAE,gBAAgB;IAChCC,KAAK,EAAE,OAAO;IACdC,WAAW,EAAE,aAAa;IAC1BC,YAAY,EAAE,sBAAsB;IACpCC,KAAK,EAAE,kBAAkB;IACzBC,MAAM,EAAE,eAAe;IACvBC,IAAI,EAAE,MAAM;IACZC,WAAW,EAAE,aAAa;IAC1BC,mBAAmB,EAAE,6BAA6B;IAClDC,gBAAgB,EAAE,0BAA0B;IAC5CC,OAAO,EAAE,gDAAgD;IACzDC,4BAA4B,EAAE,8BAA8B;IAC5DC,YAAY,EAAE,yBAAyB;IACvCC,SAAS,EAAE,oBAAoB;IAC/BC,aAAa,EAAE,wBAAwB;IACvCC,aAAa,EAAE,uCAAuC;IACtDC,YAAY,EAAE,sCAAsC;IACpDC,KAAK,EAAE,uBAAuB;IAC9BC,UAAU,EAAE,uBAAuB;IACnCC,iBAAiB,EAAE,+CAA+C;IAClEC,gBAAgB,EAAE,0CAA0C;IAC5DC,yBAAyB,EAAE,2BAA2B;IACtDC,0BAA0B,EAAE,4BAA4B;IACxDC,mBAAmB,EAAE,qBAAqB;IAC1CC,EAAE,EAAE,kBAAkB;IACtBC,GAAG,EAAE,+BAA+B;IACpCC,sBAAsB,EAAE,wBAAwB;IAChDC,gBAAgB,EAAE,kBAAkB;IACpCC,0BAA0B,EACxB,uDAAuD;IACzDC,WAAW,EAAE,kBAAkB;IAC/BC,eAAe,EAAE,uBAAuB;IACxCC,WAAW,EAAE;EACf,CAAC;EACD,MAAMC,eAAe,GAAG,CAGlB;IACEC,KAAK,EAAE,aAAa;IACpBC,WAAW,EACT,wOAAwO;IAC1OC,GAAG,EAAEC;EACP,CAAC,EACD;IACEH,KAAK,EAAE,aAAa;IACpBC,WAAW,EACT,4QAA4Q;IAC9QC,GAAG,EAAEE;EACP,CAAC,EACD;IACEJ,KAAK,EAAE,aAAa;IACpBC,WAAW,EACT,iXAAiX;IACnXC,GAAG,EAAEG;EACP,CAAC,EACD;IACEL,KAAK,EAAE,iBAAiB;IACxBC,WAAW,EACT,4TAA4T;IAC9TC,GAAG,EAAEI;EACP,CAAC,EAKD;IACEN,KAAK,EAAE,oBAAoB;IAC3BC,WAAW,EACT,mPAAmP;IACrPC,GAAG,EAAEK;EACP,CAAC,EACD;IACEP,KAAK,EAAE,oBAAoB;IAC3BC,WAAW,EACT,+SAA+S;IACjTC,GAAG,EAAEM;EACP,CAAC,EACD;IACER,KAAK,EAAE,mBAAmB;IAC1BC,WAAW,EACT,8NAA8N;IAChOC,GAAG,EAAEO;EACP,CAAC,GAGD;IACET,KAAK,EAAE,UAAU;IACjBC,WAAW,EACT,2NAA2N;IAC7NC,GAAG,EAAEQ;EACP,CAAC,EACD;IACEV,KAAK,EAAE,gBAAgB;IACvBC,WAAW,EACT,8OAA8O;IAChPC,GAAG,EAAES;EACP,CAAC,EACD;IACEX,KAAK,EAAE,gBAAgB;IACvBC,WAAW,EACT,2LAA2L;IAC7LC,GAAG,EAAEU;EACP,CAAC,EAGD;IACEZ,KAAK,EAAE,WAAW;IAClBC,WAAW,EACT,oMAAoM;IACtMC,GAAG,EAAEW;EACP,CAAC,EACD;IACEb,KAAK,EAAE,gBAAgB;IACvBC,WAAW,EACT,2MAA2M;IAC7MC,GAAG,EAAEY;EACP,CAAC,EACD;IACEd,KAAK,EAAE,OAAO;IACdC,WAAW,EACT,0LAA0L;IAC5LC,GAAG,EAAEa;EACP,CAAC,EACD;IACEf,KAAK,EAAE,aAAa;IACpBC,WAAW,EACT,oMAAoM;IACtMC,GAAG,EAAEc;EACP,CAAC,EACD;IACEhB,KAAK,EAAE,MAAM;IACbC,WAAW,EACT,sLAAsL;IACxLC,GAAG,EAAEe;EACP,CAAC,EACD;IACEjB,KAAK,EAAE,eAAe;IACtBC,WAAW,EACT,uJAAuJ;IACzJC,GAAG,EAAEgB;EACP,CAAC,EACD;IACElB,KAAK,EAAE,eAAe;IACtBC,WAAW,EACT,4QAA4Q;IAC9QC,GAAG,EAAEiB;EACP,CAAC,EACD;IACEnB,KAAK,EAAE,aAAa;IACpBC,WAAW,EACT,+UAA+U;IACjVC,GAAG,EAAEkB;EACP,CAAC,EAGD;IACEpB,KAAK,EAAE,OAAO;IACdC,WAAW,EACT,+MAA+M;IACjNC,GAAG,EAAEmB;EACP,CAAC,EACD;IACErB,KAAK,EAAE,cAAc;IACrBC,WAAW,EACT,8SAA8S;IAChTC,GAAG,EAAEoB;EACP,CAAC,EACD;IACEtB,KAAK,EAAE,IAAI;IACXC,WAAW,EACT,qNAAqN;IACvNC,GAAG,EAAEqB;EACP,CAAC,EACD;IACEvB,KAAK,EAAE,KAAK;IACZC,WAAW,EACT,4OAA4O;IAC9OC,GAAG,EAAEsB;EACP,CAAC,EACD;IACExB,KAAK,EAAE,kBAAkB;IACzBC,WAAW,EACT,sMAAsM;IACxMC,GAAG,EAAEuB;EACP,CAAC,EACD;IACEzB,KAAK,EAAE,wBAAwB;IAC/BC,WAAW,EACT,6PAA6P;IAC/PC,GAAG,EAAEwB;EACP,CAAC,EACD;IACE1B,KAAK,EAAE,QAAQ;IACfC,WAAW,EACT,0VAA0V;IAC5VC,GAAG,EAAEyB;EACP,CAAC,EAID;IACE3B,KAAK,EAAE,gBAAgB;IACvBC,WAAW,EACT,oNAAoN;IACtNC,GAAG,EAAE0B;EACP,CAAC,EACD;IACE5B,KAAK,EAAE,iBAAiB;IACxBC,WAAW,EACT,gUAAgU;IAClUC,GAAG,EAAE2B;EACP,CAAC,EACD;IACE7B,KAAK,EAAE,iBAAiB;IACxBC,WAAW,EACT,uJAAuJ;IACzJC,GAAG,EAAE4B;EACP,CAAC,EACD;IACE9B,KAAK,EAAE,SAAS;IAChBC,WAAW,EACT,6LAA6L;IAC/LC,GAAG,EAAE6B;EACP,CAAC,EAGD;IACE/B,KAAK,EAAE,qBAAqB;IAC5BC,WAAW,EACT,yRAAyR;IAC3RC,GAAG,EAAE8B;EACP,CAAC,EACD;IACEhC,KAAK,EAAE,qBAAqB;IAC5BC,WAAW,EACT,kQAAkQ;IACpQC,GAAG,EAAE+B;EACP,CAAC,EACD;IACEjC,KAAK,EAAE,oBAAoB;IAC3BC,WAAW,EACT,uUAAuU;IACzUC,GAAG,EAAEgC;EACP,CAAC,EAGD;IACElC,KAAK,EAAE,OAAO;IACdC,WAAW,EACT,gOAAgO;IAClOC,GAAG,EAAEiC;EACP,CAAC,EACD;IACEnC,KAAK,EAAE,cAAc;IACrBC,WAAW,EACT,mQAAmQ;IACrQC,GAAG,EAAEkC;EACP,CAAC,EACD;IACEpC,KAAK,EAAE,WAAW;IAClBC,WAAW,EACT,gMAAgM;IAClMC,GAAG,EAAEmC;EACP,CAAC,EACD;IACErC,KAAK,EAAE,aAAa;IACpBC,WAAW,EACT,4QAA4Q;IAC9QC,GAAG,EAAEE;EACP,CAAC,EACD;IACEJ,KAAK,EAAE,eAAe;IACtBC,WAAW,EACT,uJAAuJ;IACzJC,GAAG,EAAEgB;EACP,CAAC,EACD;IACElB,KAAK,EAAE,aAAa;IACpBC,WAAW,EACT,+UAA+U;IACjVC,GAAG,EAAEkB;EACP,CAAC,EACD;IACEpB,KAAK,EAAE,YAAY;IACnBC,WAAW,EACT,2OAA2O;IAC7OC,GAAG,EAAEoC;EACP,CAAC,EACD;IACEtC,KAAK,EAAE,mBAAmB;IAC1BC,WAAW,EACT,mPAAmP;IACrPC,GAAG,EAAEqC;EACP,CAAC,EAID;IACEvC,KAAK,EAAE,qBAAqB;IAC5BC,WAAW,EACT,6aAA6a;IAC/aC,GAAG,EAAEsC;EACP,CAAC,EACD;IACExC,KAAK,EAAE,kBAAkB;IACzBC,WAAW,EACT,qaAAqa;IACvaC,GAAG,EAAEuC;EACP,CAAC,EACD;IACEzC,KAAK,EAAE,SAAS;IAChBC,WAAW,EACT,mcAAmc;IACrcC,GAAG,EAAEwC;EACP,CAAC,EAGD;IACE1C,KAAK,EAAE,cAAc;IACrBC,WAAW,EACT,qUAAqU;IACvUC,GAAG,EAAEyC;EACP,CAAC,EACD;IACE3C,KAAK,EAAE,8BAA8B;IACrCC,WAAW,EACT,+cAA+c;IACjdC,GAAG,EAAE0C;EACP,CAAC,EAGD;IACE5C,KAAK,EAAE,kBAAkB;IACzBC,WAAW,EACT,6TAA6T;IAC/TC,GAAG,EAAE2C;EACP,CAAC,EACD;IACE7C,KAAK,EAAE,4BAA4B;IACnCC,WAAW,EACT,gTAAgT;IAClTC,GAAG,EAAE4C;EACP,CAAC,EACD;IACE9C,KAAK,EAAE,2BAA2B;IAClCC,WAAW,EACT,6RAA6R;IAC/RC,GAAG,EAAE6C;EACP,CAAC,EACD;IACE/C,KAAK,EAAE,4BAA4B;IACnCC,WAAW,EACT,yRAAyR;IAC3RC,GAAG,EAAE8C;EACP,CAAC,CAEF;EACL,oBAEEvG,OAAA;IAAAwG,QAAA,eACIxG,OAAA,CAACH,QAAQ;MAAC4G,SAAS,EAAC,QAAQ;MAAAD,QAAA,gBAC5BxG,OAAA,CAACH,QAAQ,CAAC6G,MAAM;QAACD,SAAS,EAAC,eAAe;QAACE,OAAO,EAAC,SAAS;QAACC,EAAE,EAAC,gBAAgB;QAAAJ,QAAA,EAE1ErG,QAAQ,IAAI,IAAI,GAAG,oBAAoB,GAAGC,SAAS,CAACD,QAAQ;MAAC;QAAA0G,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAGlD,CAAC,eAElBhH,OAAA,CAACH,QAAQ,CAACoH,IAAI;QAACR,SAAS,EAAC,eAAe;QAAAD,QAAA,EACvCU,MAAM,CAACC,OAAO,CAAC/G,SAAS,CAAC,CAACgH,GAAG,CAACC,KAAA,IAAiB;UAAA,IAAhB,CAACC,IAAI,EAAEC,KAAK,CAAC,GAAAF,KAAA;UAC3C,oBAAOrH,OAAA,CAACH,QAAQ,CAAC2H,IAAI;YAAaf,SAAS,EAAEtG,QAAQ,KAAGmH,IAAI,GAAE,QAAQ,GAAG,EAAG;YAAAd,QAAA,eAC5ExG,OAAA,CAACF,IAAI;cAAC2H,EAAE,EAAE,cAAc,GAACH,IAAK;cAACb,SAAS,EAAC,cAAc;cAAAD,QAAA,eACrDxG,OAAA;gBAAAwG,QAAA,EACCe;cAAK;gBAAAV,QAAA,EAAAC,YAAA;gBAAAC,UAAA;gBAAAC,YAAA;cAAA,OACA;YAAC;cAAAH,QAAA,EAAAC,YAAA;cAAAC,UAAA;cAAAC,YAAA;YAAA,OAEH;UAAC,GANoBM,IAAI;YAAAT,QAAA,EAAAC,YAAA;YAAAC,UAAA;YAAAC,YAAA;UAAA,OAOhB,CAAC;QAClB,CAAC;MAAC;QAAAH,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAOa,CAAC;IAAA;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACN;EAAC;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OACV,CAAC;AAEV;AAACU,EAAA,GA9cQzH,YAAY;AAgdrB,eAAeA,YAAY;AAAA,IAAAyH,EAAA;AAAAC,YAAA,CAAAD,EAAA"},"metadata":{},"sourceType":"module","externalDependencies":[]}